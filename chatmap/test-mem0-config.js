/**
 * Test script to verify mem0ai configuration with Qdrant and Ollama
 * Run with: node test-mem0-config.js
 */

const { MemoryClient } = require('mem0ai');

async function testMem0Config() {
  console.log('üß™ Testing mem0ai configuration...\n');

  try {
    // Test configuration
    const config = {
      // Vector database configuration
      vectorDB: {
        provider: 'qdrant',
        config: {
          url: 'http://localhost:6333',
          collectionName: 'chatmap_memories_test',
        },
      },
      // Embedding configuration
      embedding: {
        provider: 'ollama',
        config: {
          model: 'nomic-text:latest',
          baseUrl: 'http://localhost:11434',
        },
      },
      // LLM configuration
      llm: {
        provider: 'ollama',
        config: {
          model: 'llama3.2:3b',
          baseUrl: 'http://localhost:11434',
        },
      },
    };

    console.log('üìã Configuration:');
    console.log(JSON.stringify(config, null, 2));
    console.log('');

    // Initialize MemoryClient
    console.log('üîß Initializing MemoryClient...');
    const client = new MemoryClient({ config });
    console.log('‚úÖ MemoryClient initialized successfully\n');

    // Test adding a memory
    console.log('üíæ Testing memory operations...');
    const memoryId = await client.addMemory({
      messages: [
        {
          role: 'user',
          content: 'I love Italian food and prefer restaurants near downtown'
        }
      ],
      metadata: {
        userId: 'test-user',
        type: 'preference',
        location: 'downtown'
      }
    });
    console.log(`‚úÖ Memory added with ID: ${memoryId}\n`);

    // Test searching memories
    console.log('üîç Testing memory search...');
    const searchResults = await client.searchMemories({
      query: 'Italian food preferences',
      limit: 5
    });
    console.log(`‚úÖ Found ${searchResults.length} memories`);
    console.log('Search results:', JSON.stringify(searchResults, null, 2));
    console.log('');

    // Test getting memories
    console.log('üìñ Testing get memories...');
    const memories = await client.getMemories({
      limit: 10
    });
    console.log(`‚úÖ Retrieved ${memories.length} memories`);
    console.log('');

    console.log('üéâ All tests passed! mem0ai is configured correctly.');
    console.log('\nYour setup:');
    console.log('  - Vector DB: Qdrant ‚úÖ');
    console.log('  - Embeddings: Ollama nomic-text ‚úÖ');
    console.log('  - LLM: Ollama llama3.2:3b ‚úÖ');

  } catch (error) {
    console.error('‚ùå Test failed:', error.message);
    console.error('\nTroubleshooting:');
    console.error('1. Make sure Qdrant is running: docker-compose up -d');
    console.error('2. Make sure Ollama is running: ollama serve');
    console.error('3. Make sure models are installed:');
    console.error('   - ollama pull nomic-text:latest');
    console.error('   - ollama pull llama3.2:3b');
    console.error('4. Check if ports are available:');
    console.error('   - Qdrant: http://localhost:6333');
    console.error('   - Ollama: http://localhost:11434');
  }
}

// Run the test
testMem0Config();
